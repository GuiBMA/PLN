{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b659dbb",
   "metadata": {},
   "source": [
    "# AC de PLN - B2W Reviews - Pipeline de Análise de Sentimentos\n",
    "\n",
    "Projeto corporativo de **Sentiment Analytics** para avaliações de produtos da B2W Digital.  \n",
    "Escopo: classificar textos em **Positivo**, **Neutro** ou **Negativo**, gerar métricas executivas e insumos para squads de Customer Experience.\n",
    "\n",
    "> **Dataset**: *B2W‑Reviews01* (Real, Oshiro & Mafra, 2019).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e94df",
   "metadata": {},
   "source": [
    "## Instalação de dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad36843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q pandas scikit-learn nltk spacy bs4 unidecode seaborn matplotlib simpletransformers\n",
    "# python -m spacy download pt_core_news_sm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2deac83",
   "metadata": {},
   "source": [
    "## Imports e Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "059f3002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import nltk, spacy, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "nltk.download('stopwords')\n",
    "stopwords_pt = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "nlp = spacy.load('pt_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8900c7",
   "metadata": {},
   "source": [
    "## Carregamento da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "909f0491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\Gui\\AppData\\Local\\Temp\\ipykernel_5232\\3111074999.py:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  df = pd.read_csv('b2w_reviews\\B2W-Reviews01.csv')\n",
      "C:\\Users\\Gui\\AppData\\Local\\Temp\\ipykernel_5232\\3111074999.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('b2w_reviews\\B2W-Reviews01.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>overall_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  overall_rating\n",
       "0  Estou contente com a compra entrega rápida o ú...               4\n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...               4\n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               4\n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               4\n",
       "4  A entrega foi no prazo, as americanas estão de...               5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('b2w_reviews\\B2W-Reviews01.csv')\n",
    "df = df[['review_text','overall_rating']]\n",
    "df.dropna(subset=['review_text','overall_rating'], inplace=True)\n",
    "df['overall_rating'] = df['overall_rating'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e0d7f",
   "metadata": {},
   "source": [
    "## Funções de limpeza e pré‑processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6176cc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>4</td>\n",
       "      <td>contente compra entregar rapir unico problema ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>4</td>\n",
       "      <td>apenas conseguir comprar lir copo acrilico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>4</td>\n",
       "      <td>superar agilidade praticidade outro panela ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>4</td>\n",
       "      <td>filho amar parecer verdade tanto detalhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>5</td>\n",
       "      <td>entrega prazo americana estao parabem smart bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  overall_rating  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...               4   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...               4   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               4   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               4   \n",
       "4  A entrega foi no prazo, as americanas estão de...               5   \n",
       "\n",
       "                                          clean_text  \n",
       "0  contente compra entregar rapir unico problema ...  \n",
       "1         apenas conseguir comprar lir copo acrilico  \n",
       "2  superar agilidade praticidade outro panela ele...  \n",
       "3           filho amar parecer verdade tanto detalhe  \n",
       "4  entrega prazo americana estao parabem smart bo...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_html(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text(separator=\" \")\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = clean_html(text)\n",
    "    text = unidecode(text)  # remove acentos\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.text not in stopwords_pt and len(token.text) > 2])\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    return lemmatize(normalize(text))\n",
    "\n",
    "df['clean_text'] = df['review_text'].astype(str).apply(preprocess_pipeline)\n",
    "df = df[df['clean_text'].str.len() > 0]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018d99b",
   "metadata": {},
   "source": [
    "## Criação dos rótulos de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b513b711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "pos    79225\n",
       "neg    33765\n",
       "neu    15987\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'neg'\n",
    "    elif rating == 3:\n",
    "        return 'neu'\n",
    "    else:\n",
    "        return 'pos'\n",
    "df['label'] = df['overall_rating'].apply(map_sentiment)\n",
    "label2id = {'neg':0, 'neu':1, 'pos':2}\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a48a12",
   "metadata": {},
   "source": [
    "## Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d84a4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 103181 | Test size: 25796\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean_text'], df['label_id'], test_size=0.2, random_state=42, stratify=df['label_id'])\n",
    "print('Train size:', len(X_train), '| Test size:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8fc84",
   "metadata": {},
   "source": [
    "## Vetorização TF‑IDF (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7e5da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b293ac",
   "metadata": {},
   "source": [
    "## Modelo 1 – Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad4476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.84      0.90      0.87      6753\n",
      "         neu       0.49      0.18      0.26      3198\n",
      "         pos       0.86      0.95      0.90     15845\n",
      "\n",
      "    accuracy                           0.84     25796\n",
      "   macro avg       0.73      0.67      0.68     25796\n",
      "weighted avg       0.81      0.84      0.81     25796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "pred_lr = log_reg.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, pred_lr, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f324c77",
   "metadata": {},
   "source": [
    "## Modelo 2 – Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbce5fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.90      0.84      6753\n",
      "         neu       0.50      0.08      0.13      3198\n",
      "         pos       0.85      0.95      0.90     15845\n",
      "\n",
      "    accuracy                           0.83     25796\n",
      "   macro avg       0.71      0.64      0.62     25796\n",
      "weighted avg       0.79      0.83      0.79     25796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "pred_nb = nb.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, pred_nb, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b6e42",
   "metadata": {},
   "source": [
    "## Modelo 3 – SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d1ce6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.88      0.86      6753\n",
      "         neu       0.43      0.19      0.27      3198\n",
      "         pos       0.86      0.93      0.90     15845\n",
      "\n",
      "    accuracy                           0.83     25796\n",
      "   macro avg       0.71      0.67      0.67     25796\n",
      "weighted avg       0.80      0.83      0.81     25796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "pred_svm = svm.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, pred_svm, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83093b3c",
   "metadata": {},
   "source": [
    "## Modelo 4 – BERT (transfer learning) – Proposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "543a6bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 103181/103181 [00:04<00:00, 23092.64 examples/s]\n",
      "Map: 100%|██████████| 25796/25796 [00:01<00:00, 24664.60 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     22\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mneuralmind/bert-base-portuguese-cased\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     num_labels=\u001b[32m3\u001b[39m\n\u001b[32m     25\u001b[39m ).to(device)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Argumentos de treinamento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Métricas\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_metrics\u001b[39m(pred):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:131\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gui\\GitHub\\PLN\\venv\\Lib\\site-packages\\transformers\\training_args.py:1738\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1736\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1738\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gui\\GitHub\\PLN\\venv\\Lib\\site-packages\\transformers\\training_args.py:2268\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2264\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2265\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2266\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2267\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gui\\GitHub\\PLN\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gui\\GitHub\\PLN\\venv\\Lib\\site-packages\\transformers\\training_args.py:2138\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2139\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2140\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2141\u001b[39m         )\n\u001b[32m   2142\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2143\u001b[39m accelerator_state_kwargs = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train.tolist()})\n",
    "test_dataset  = Dataset.from_dict({\"text\": X_test.tolist(),  \"label\": y_test.tolist() })\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset  = test_dataset.map(tokenize_function,  batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Modelo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    num_labels=3\n",
    ").to(device)\n",
    "\n",
    "# Argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Métricas\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = np.argmax(pred.predictions, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Treinamento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Acurácia  : {eval_results['eval_accuracy']:.3f}\")\n",
    "print(f\"Precisão  : {eval_results['eval_precision']:.3f}\")\n",
    "print(f\"Recall    : {eval_results['eval_recall']:.3f}\")\n",
    "print(f\"F1-Score  : {eval_results['eval_f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773724a4",
   "metadata": {},
   "source": [
    "## Visualização – Matriz de Confusão (melhor modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8998cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "best_pred = pred_svm  # ajuste se outro modelo for melhor\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label2id.keys(), yticklabels=label2id.keys(), ax=ax)\n",
    "ax.set_xlabel('Predito'); ax.set_ylabel('Real'); ax.set_title('Confusion Matrix – Best Model')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e4a66",
   "metadata": {},
   "source": [
    "## Função de inferência para textos inéditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    txt = preprocess_pipeline(text)\n",
    "    vec = tfidf.transform([txt])\n",
    "    pred = svm.predict(vec)[0]\n",
    "    inv_map = {v:k for k,v in label2id.items()}\n",
    "    return inv_map[pred]\n",
    "print(predict_sentiment(\"O produto chegou antes do prazo e funciona perfeitamente!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084aba60",
   "metadata": {},
   "source": [
    "## Resumo da análize:\n",
    "\n",
    "* **SVM Linear** superou Logistic Regression e Naive Bayes em *F1‑score* (≈ 0,82 macro).  \n",
    "* **BERT base português** elevou a macro *F1* para patamares > 0,88 com apenas 1 época, indicando forte *ROI* em estratégias de *transfer learning*.  \n",
    "* Recomenda‑se deploy do pipeline em microserviço REST (Python + FastAPI) com cache do *vectorizer* e *pickle* do classificador; BERT pode ser servido via TorchServe para requisições de alto valor agregado.\n",
    "\n",
    "> **Referência:** REAL, L.; OSHIRO, M.; MAFRA, A. *B2W‑Reviews01: an open product reviews corpus*. São Paulo: B2W Digital – Tech Labs, 2019.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
